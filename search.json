[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Tarea 1.html",
    "href": "Tarea 1.html",
    "title": "2  Tarea 1",
    "section": "",
    "text": "2.1 Exercise 1\nExercise 1 Read (Sec 1.1, pp 1-2 ) and answer the following. Explain why Reinforcement Learning differs for supervised and unsupervised learning.\nEn el aprendizaje reforzado un agente aprende a tomar decisiones (acciones) a través de la interacción con su entorno y recibiendo recompensas o castigos en función de las mismas, a diferencia del aprendizaje supervisado, ya que en este tipo de aprendizaje automático, un modelo se entrena utilizando un conjunto de datos que incluye tanto las entradas como las salidas correspondientes (etiquetas). es decir, consiste en aprender a partir de un conjunto de ejemplos ya etiquetados y proporcionados por un supervisor externo con conocimientos. Por lo que en este tipo de aprendizaje cada ejemplo describe una situación especifica, y además existe una etiqueta que indica la acción adecuada que el sistema debe tomar en esa situación. El objetivo de este tipo de aprendizaje es que el sistema generalice sus respuestas para que actúe correctamente en situaciones que no están presentes en el conjunto de entrenamiento. Por otra parte el aprendizaje por refuerzo también es diferente del aprendizaje no supervisado, que generalmente consiste en encontrar estructuras ocultas en conjuntos de datos no etiquetados y aunque en parte el aprendizaje por refuerzo es un tipo de aprendizaje no supervisado, en realidad este se centra mas que nada en maximizar una recompensa en lugar de buscar patrones ocultos en los datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 1.html#exercise-2",
    "href": "Tarea 1.html#exercise-2",
    "title": "2  Tarea 1",
    "section": "2.2 Exercise 2",
    "text": "2.2 Exercise 2\nExercise 2 See the first Steve Bruton’s youtube video about Reinforcement Learning. Then accordingly to its presentation explain what is the meaning of the following expression: \\[V_{\\pi}(s)=E\\left(\\sum_{t}\\gamma^tr_t|s_0=s \\right)\\]\nEsta expresión es una función con la cual se mide el desempeño del sistema bajo diferentes políticas de control dado el estado inicial, es decir, nos ayuda a identificar que acciones fueron buenas y cuales fueron malas, además dicha expresión nos da el valor esperado de cuanta recompensa obtendremos en un futuro al elegir dicha politica dado un estado inicial. Por otra parte, el factor de descuento en la expresión, nos ayuda a comparar las recompensas futuras con las recompensas inmediatas, basicamente nos dice que tan a favor estamos de obtener una recompensa en el estado actual frente a un futuro lejano",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 1.html#exercise-3",
    "href": "Tarea 1.html#exercise-3",
    "title": "2  Tarea 1",
    "section": "2.3 Exercise 3",
    "text": "2.3 Exercise 3\nExercise 3 Form (see ) obtain a time line pear year from 1950 to 2012.\n\nlibrary(bibtex)\n## Activate the Core Packages\nlibrary(tidyverse) ## Brings in a core of useful functions\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nlibrary(gt)        ## Tables\n## Specific packages\nlibrary(milestones)\n## Initialize defaults\n## Initialize defaults\ncolumn &lt;- lolli_styles()\n\ndata &lt;- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')\n\n\n## Sort the table by date\ndata &lt;- data |&gt;\n  arrange(date)\n\n## Build a table\ngt(data) |&gt;\n  #cols_hide(columns = event) |&gt;\n  tab_style(cell_text(v_align = \"top\"),\n            locations = cells_body(columns = date)) |&gt;\n  tab_source_note(source_note = \"Source: Sutton and Barto (2018)\") \n\n\n\n\n\n\n\ndate\nevent\nreference\n\n\n\n\n1911\nThe first idea of \"trial-error-learning\" emerges.\nThorndike, E. L. (1911). Animal Intelligence. Hafner, Darien, CT\n\n\n1927\nThe term \"Reinforcement\" appears.\nPavlov, P. I. (1927). Conditioned Re exes. Oxford University Press, London\n\n\n1954\nImportant works on \"trial-error-learning.\"\nMinsky, M. L. (1954). Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem. Ph.D. thesis, Princeton Uni versity\n\n\n1957\nDynamic Programming\"appears.\nBellman, R. E. (1957a). Dynamic Programming. Princeton University Press, Princeton.\n\n\n1957\nMarkov Decision Processes emerge.\nBellman, R.E. (1957). A Markovian Decision Process. Indiana University mathematics journal, 6(4), 679?684. https://doi.org/10.1512/iumj.1957.6.56038\n\n\n1959\nThe term \"Optimal Control\" appears.\nNA\n\n\n1959\nTemporal-difference-learning appears.\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal on Research and Development, 3:211229. Reprinted in E. A. Feigenbaum and J. Feldman (eds.), Computers and Thought, pp. 71105. McGraw-Hill, New York, 1963.\n\n\n1960\nThe policy iteration method is introduced.\nHoward, R. A. (1960). Dynamic programming and Markov processes. MIT Press.\n\n\n1960\nThe terms \"Reinforcement\" and \"Reinforcement Learning\" are used.\nWaltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning control systems. IEEE Transactions on Automatic Control, 10:390398.\n\n\n1960\n\"Learning Automata\" originate.\nTsetlin, M. L. (1973). Automaton Theory and Modeling of Biological Systems. Academic Press, New York\n\n\n1961\nPublication of \"Steps Toward Artificial Intelligence.\"\nMinsky, M. L. (1961). Steps toward arti cial intelligence. Proceedings of the Institute of Radio Engineers, 49:830. Reprinted in E. A. Feigenbaum and J. Feldman (eds.), Computers and Thought, pp. 406450. McGraw-Hill, New York, 1963.\n\n\n1963\nSTeLLA is created.\nAndreae, J. H. (1963). STELLA: A scheme for a learning machine. In Proceedings of the 2nd IFAC Congress, Basle, pp. 497502. Butterworths, London.\n\n\n1972\nTrial-error-learning and temporal-difference-learning are combined.\nKlopf, A. H. (1972). Brain function and adaptive systems A heterostatic theory. Technical Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A summary appears in Proceedings of the In ternational Conference on Systems, Man, and Cybernetics. IEEE Systems, Man, and Cybernetics Society, Dallas, TX, 1974.\n\n\n1977\nOptimal Control and Dynamic Programming are connected.\nWerbos, P. J. (1977). Advanced forecasting methods for global crisis warning and models of intelligence. General Systems Yearbook, 22:2538.\n\n\n1978\nSutton develops Klopf's ideas.\nSutton, R. S. (1978a). Learning theory support for a single channel theory of the brain. Unpublished report.\n\n\n1986\nClassifiers are introduced.\nHolland, J. H. (1986). Escaping brittleness: The possibility of general-purpose learning algorithms applied to rule-based systems. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (eds.), Machine Learning: An Arti cial Intelligence Approach, vol. 2, pp. 593623. Morgan Kaufmann, San Mateo, CA\n\n\n1989\nLearning methods are integrated.\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, Cambridge University.\n\n\n2003\nReinforcement Learning in economics.\nCamerer, C. (2003). Behavioral game theory: Experiments in strategic inter action. Princeton University Press\n\n\n2012\nReinforcement Learning and Games.\nNowe, A., Vrancx, P., De Hauwere, Y. M. (2012). Game theory and multi agent reinforcement learning. In Reinforcement Learning (pp. 441-470). Springer Berlin Heidelberg\n\n\n\nSource: Sutton and Barto (2018)\n\n\n\n\n\n\n\n\n\n## Adjust some defaults\ncolumn$color &lt;- \"black\"\ncolumn$size  &lt;- 15\ncolumn$background_color &lt;- \"pink\"\ncolumn$source_info &lt;- \"Source: Sutton and Barto (2018)\"\n\n## Milestones timeline\nmilestones(datatable = data, styles = column)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 1.html#exercise-4",
    "href": "Tarea 1.html#exercise-4",
    "title": "2  Tarea 1",
    "section": "2.4 Exercise 4",
    "text": "2.4 Exercise 4\nExercise 4 Consider the following consumption-saving problem with dynamics \\[x_{k+1}=(1+r)(x_k-a_k),\\hspace{0.5cm}k=0,1,...,N-1,\\] and utility function \\[\\beta^N(x_N)^{1-\\gamma}+\\sum_{k=0}^{N-1}\\beta^k(a_k)^{1-\\gamma}\\]. Show that the value functions of the DP algorithm take the form \\[J_k(x)=A_k\\beta^kx^{1-\\gamma},\\] where \\(A_N=1\\) and for \\(k=N-1,...,0,\\) \\[A_k=\\left[1+((1+r)\\beta A_{k+1})^{\\frac{1}{\\gamma}} \\right]^{\\gamma}\\] Show also that the optimal policies are \\(h_k(x)=A_k^{-1/\\gamma} x,\\) for \\(k=N-1,\\ldots,0\\).\nSolución\nDel algoritmo de la programación dinámica se sigue que para este caso particular \\(J_{N}(x)=\\beta^{N}(x_N)^{1-\\gamma}\\)\nluego, para \\(k= N-1\\) \\[J_{N-1}=\\min_{a\\in A(x)}\\{\\beta^{N-1}(a)^{1-\\gamma} + \\beta^{N}(1+r)^{1-\\gamma}(x-a)^{1-\\gamma}\\}\\] derivando con respecto a \\(a\\) obtenemos \\[(1-\\gamma)\\beta^{N-1}a^{-\\gamma}- \\beta^{N}(1+r)^{1-\\gamma}(x-a)^{-\\gamma}\\] después igualando a cero, obtenemos \\[(1-\\gamma)\\beta^{N-1}a^{-\\gamma}-\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] entonces \\[(\\cfrac{x-a}{a})^\\gamma=\\beta(1+r)^{1-\\gamma}\\] \\[\\cfrac{x-a}{a}=[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\] por lo que el mínimo se alcanza en \\[a_0=\\dfrac{x}{[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1}\\] sustituimos y obtenhemos la siguiente expresión para \\(J_{N--1}\\) \\[J_{N-1}(x)=\\beta^{N-1}(a_0)^{1-\\gamma}+\\beta^N(1+r)^{1-\\gamma}(x-a_0)^{1-\\gamma}\\] Luego, \\[J_{N-1}(x)=\\dfrac{\\beta^{N-1}x^{1-\\gamma}}{([\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1)^{1-\\gamma}}+\\beta^N(1+r)^{1-\\gamma}\\left[\\dfrac{x[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}}{\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1}\\right]^{1-\\gamma}\\] entonces \\[J_{N-1}(x)=\\beta^{N-1}x^{1-\\gamma}\\left[[\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}+1\\right]^{\\gamma}\\] finalmente \\[J_{N-1}(x)=A_{N-1}\\beta^{N-1}x^{1-\\gamma}\\] donde \\[A_{N-1}=\\left(1+((1+r)^{1-\\gamma}\\beta)^{\\frac{1}{\\gamma}}\\right)^{\\gamma}\\] Supongamos que se cumple para \\(n=k+1\\), entonces \\(J_{k+1}(x)=A_{k+1}\\beta^{k+1}x^{1-\\gamma}\\). Ahora \\[J_k(x)=\\min_{0&lt;a&lt;x}\\left\\{\\beta^ka^{1-\\gamma}+A_{k+1}\\beta^{k+1}(1+r)^{1-\\gamma}(x-a)^{1-\\gamma} \\right\\}\\] Derivando e igualando a 0 para encontrar el mínimo \\[(1-\\gamma)\\beta^ka^{-\\gamma}-A_{k+1}\\beta^{k+1}(1+\\gamma)^{1-\\gamma}(1-\\gamma)(x-a)^{-\\gamma}=0\\] Notemos que \\[(1-\\gamma)\\beta^k\\left[ a^{-\\gamma}-A_{k+1}\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}  \\right]=0\\] \\[a^{-\\gamma}-A_{k+1}\\beta(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] entonces \\[\\left(\\dfrac{x-a}{a}\\right)^{\\gamma}=A_{k+1}\\beta(1+\\gamma)^{1-\\gamma}\\] Despejando \\(a\\), obtenemos que el mínimo se alcanza en \\[a=\\dfrac{x}{\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1}\\] A este punto lo renombramos como \\(a_0\\) y sustituimos \\[J_k(x)=\\dfrac{\\beta^kx^{1-\\gamma}}{\\left[\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1\\right]^{1-\\gamma}}+\\dfrac{A_{k+1}\\beta^{k+1}(1+r)^{1-\\gamma}\\left(x[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}}{\\left[\\left[ A_{k+1}\\beta(1+r)^{1-\\gamma}  \\right]^{\\frac{1}{\\gamma}}+1\\right]^{1-\\gamma}}\\] Desarrollando y reacomodando términos llegamos a \\[\\dfrac{\\beta^kx^{1-\\gamma}\\left( 1+[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}  \\right)}{\\left(1+[A_{k+1}\\beta(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}  \\right)^{1-\\gamma}}=\\beta^kx^{1-\\gamma}A_k\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 1.html#exercise-5",
    "href": "Tarea 1.html#exercise-5",
    "title": "2  Tarea 1",
    "section": "2.5 Exercise 5",
    "text": "2.5 Exercise 5\nExercise 5 Consider now the infinite–horizon version of the above consumption–saving problem.\n\nWrite down the associated Bellman equation.\nArgue why a solution to the Bellman equation should be of the form \\[v(x)=cx^{1-\\gamma}\\], where \\(c\\) is constant. Find the constant and the stationary optimal policy.\n\nSolución\n\\[cx^{1-\\gamma}=\\min\\left\\{ a^{1-\\gamma}+\\beta c(1+r)^{1-\\gamma}(x-a)^{1-\\gamma} \\right\\}\\] Derivando respecto a \\(a\\) e igualando a 0 \\[(1-\\gamma)a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(1-\\gamma)(x-a)^{-\\gamma}=0\\] \\[(1-\\gamma)\\left[ a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(x-a)^{-\\gamma}\\right]=0\\] \\[a^{-\\gamma}-\\beta c(1+r)^{1-\\gamma}(x-a)^{-\\gamma}=0\\] Despenjando \\(a\\) \\[\\left(\\dfrac{x-a}{a}\\right)^{\\gamma}=\\beta c(1+r)^{1-\\gamma}\\] \\[x=a\\left[\\beta c(1+r)^{1-\\gamma} \\right]^{\\frac{1}{\\gamma}}+a\\] \\[a_0=a=\\dfrac{x}{\\left[\\beta c(1+r)^{1-\\gamma} \\right]^{\\frac{1}{\\gamma}}+1}\\] Sustituyendo \\(a_0\\), así \\[cx^{1-\\gamma}=\\dfrac{x^{1-\\gamma}+\\beta c(1+r)^{1-\\gamma}x^{1-\\gamma}\\left[ (\\beta c (1+r)^{1-\\gamma})^{\\frac{1}{\\gamma}} \\right]^{1-\\gamma}}{\\left[ (\\beta c (1+r)^{1-\\gamma})^{\\frac{1}{\\gamma}} +1\\right]^{1-\\gamma}}\\] \\[cx^{1-\\gamma}=x^{1-\\gamma}\\left[1+[\\beta c (1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] entonces \\[cx^{1-\\gamma}=x^{1-\\gamma}\\left[1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] \\[c=\\left[1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\right]^{\\gamma}\\] \\[c^{\\frac{1}{\\gamma}}=1+[\\beta c(1+r)^{1-\\gamma}]^{\\frac{1}{\\gamma}}\\] \\[c^{\\frac{1}{\\gamma}}=\\left[1-\\beta^{\\frac{1}{\\gamma}}(1+r)^{\\frac{1-\\gamma}{\\gamma}}\\right]\\] \\[c^{\\frac{1}{\\gamma}}=\\dfrac{1}{1-\\beta^{\\frac{1}{\\gamma}}(1+r)^{\\frac{1-\\gamma}{\\gamma}}}\\] de donde \\[\nc=\\left[\\dfrac{1}{1-\\beta^{\\frac{1}{\\gamma}}(1+r)^{\\frac{1-\\gamma}{\\gamma}}}\\right]^{\\gamma}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "Tarea 1.html#exercise-6",
    "href": "Tarea 1.html#exercise-6",
    "title": "2  Tarea 1",
    "section": "2.6 Exercise 6",
    "text": "2.6 Exercise 6\nExercise 6 Let \\(\\{\\xi_k\\}\\) be a sequence of iid random variables such that \\(E[\\xi]=0\\) and \\(E[\\xi^2]=d\\). Consider the dynamics \\[x_{k+1}=x_k+a_k+\\xi_k,\\hspace{0.5cm}k=0,1,2,...,\\] and the discounted cost \\[E\\sum \\beta^k(a_k^2+x_k^2).\\] i. Write down the associated Bellman equation.\n\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) and \\(b\\) are constant.\nDetermine the constants \\(a\\) and \\(b\\).\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) y \\(b\\) are constant. Determine the constants \\(a\\) and \\(b\\). Solución Sea \\(A=a\\) y \\(B=b\\), entonces \\[Ax^2+B=\\min_{a\\in A(x)}\\{a^2+x^2+\\beta E[A(x+a+\\xi)^2+B]\\}\\] \\[=\\min_{a\\in A(x)}\\{a^2+x^2+\\beta AE[(x+a+\\xi)^2]+\\beta B\\}\\] \\[=\\min_{a\\in A(x)}\\{a^2+x^2+A\\beta E[x^2+2ax+a^2+2(x+a)\\xi+\\xi^2]+\\beta B\\}\\] \\[=\\min_{a\\in A(x)}\\{a^2+x^2+A\\beta x^2+2axA\\beta+A\\beta a^2+A\\beta d+\\beta B\\}\\] Derivando respecto a \\(a\\) \\[2a+2xA\\beta+2A\\beta a=0\\] Despejando, obtenemos que el mínimo se alcanza en \\[a=\\dfrac{-xA\\beta}{1+A\\beta}\\] Entonces \\[Ax^2+B=\\dfrac{(xA\\beta)^2}{(a+A\\beta)^2}+x^2+\\beta E\\left[A\\left(\\dfrac{x}{1+A\\beta}+\\xi \\right)^2 \\right]+\\beta B\\] \\[=\\dfrac{x^2A^2\\beta^2}{(1+A\\beta)^2}+x^2+A\\beta E\\left[ \\dfrac{x^2}{(1+A\\beta)^2}+\\dfrac{2x\\xi}{1+A\\beta}+\\xi^2\\right]+\\beta B\\]\n\n\\[=\\dfrac{x^2A\\beta(1+A\\beta)}{(1+A\\beta)^2}+x^2+A\\beta d+\\beta B\\] \\[=x^2\\left(1+\\dfrac{A\\beta}{1+A\\beta}\\right)+A\\beta d+\\beta B\\] Por lo que \\[A=1+\\dfrac{A\\beta}{1+A\\beta}\\] \\[B=\\dfrac{A\\beta d}{1-\\beta}\\] Ahora \\[A=\\dfrac{1+2A\\beta}{1+A\\beta}\\] \\[A^2\\beta+A(1-2\\beta)-1=0\\] Resolviendo \\[A=\\dfrac{-1+2\\beta\\pm \\sqrt{4\\beta^2+1}}{2\\beta}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tarea 1</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]