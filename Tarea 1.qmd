# Tarea 1


## Exercise 1
**Exercise 1** Read (Sec 1.1, pp 1-2 ) and answer the following.
Explain why Reinforcement Learning differs for supervised and unsupervised learning.

*En el aprendizaje reforzado un agente aprende a tomar decisiones (acciones) a través de la interacción con su entorno y recibiendo recompensas o castigos en función de las mismas, a diferencia del aprendizaje supervisado, ya que en este tipo de aprendizaje automático, un modelo se entrena  utilizando un conjunto de datos que incluye tanto las entradas como las salidas correspondientes (etiquetas). es decir, consiste en aprender a  partir de un  conjunto  de  ejemplos ya etiquetados y proporcionados  por  un  supervisor  externo  con  conocimientos. Por lo que en este tipo de aprendizaje cada ejemplo describe una situación especifica, y además existe una etiqueta que indica la acción adecuada que el sistema debe tomar en  esa  situación. El  objetivo  de  este  tipo de  aprendizaje  es  que  el sistema  generalice  sus  respuestas  para  que  actúe  correctamente  en  situaciones  que  no  están presentes  en  el  conjunto  de  entrenamiento. Por otra parte el aprendizaje  por  refuerzo  también  es  diferente  del aprendizaje  no  supervisado, que  generalmente  consiste  en encontrar  estructuras  ocultas en conjuntos de  datos  no  etiquetados y aunque en parte el aprendizaje por refuerzo es un tipo de aprendizaje no supervisado, en realidad  este se centra  mas que nada en maximizar una recompensa en lugar de buscar patrones ocultos en los datos.*

## Exercise 2
**Exercise 2** See the first Steve Bruton's youtube video about [Reinforcement Learning](https://www.youtube.com/watch?v=0MNVhXEX9to&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74&ab_channel=SteveBrunton). Then accordingly to its presentation explain what is the meaning of the following expression:
$$V_{\pi}(s)=E\left(\sum_{t}\gamma^tr_t|s_0=s \right)$$

*Esta expresión es una función con la cual se mide el desempeño del sistema bajo diferentes políticas de control dado el estado inicial, es decir, nos ayuda a identificar que acciones fueron buenas y cuales fueron malas, además dicha expresión nos da el valor esperado de cuanta recompensa obtendremos en un futuro al elegir dicha politica dado un estado inicial. Por otra parte, el factor de descuento en la expresión, nos ayuda a comparar las recompensas futuras con las recompensas inmediatas, basicamente nos dice que tan a favor estamos de obtener una recompensa en el estado actual frente a un futuro lejano*


## Exercise 3
**Exercise 3** Form (see ) obtain a time line pear year from 1950 to 2012.
```{r}
library(bibtex)
## Activate the Core Packages
library(tidyverse) ## Brings in a core of useful functions
```

```{r}
library(gt)        ## Tables
## Specific packages
library(milestones)
## Initialize defaults
## Initialize defaults
column <- lolli_styles()

data <- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')
```

```{r}
## Sort the table by date
data <- data |>
  arrange(date)

## Build a table
gt(data) |>
  #cols_hide(columns = event) |>
  tab_style(cell_text(v_align = "top"),
            locations = cells_body(columns = date)) |>
  tab_source_note(source_note = "Source: Sutton and Barto (2018)") 
```


```{r}
## Adjust some defaults
column$color <- "black"
column$size  <- 15
column$background_color <- "pink"
column$source_info <- "Source: Sutton and Barto (2018)"

## Milestones timeline
milestones(datatable = data, styles = column)
```




## Exercise 4
**Exercise 4** Consider the following **consumption-saving** problem with dynamics
$$x_{k+1}=(1+r)(x_k-a_k),\hspace{0.5cm}k=0,1,...,N-1,$$
and utility function
$$\beta^N(x_N)^{1-\gamma}+\sum_{k=0}^{N-1}\beta^k(a_k)^{1-\gamma}$$.
Show that the value functions of the DP algorithm take the form
$$J_k(x)=A_k\beta^kx^{1-\gamma},$$
where $A_N=1$ and for $k=N-1,...,0,$
$$A_k=\left[1+((1+r)\beta A_{k+1})^{\frac{1}{\gamma}} \right]^{\gamma}$$
Show also that the optimal policies are $h_k(x)=A_k^{-1/\gamma} x,$ for $k=N-1,\ldots,0$.

*Solución*

Del algoritmo de la programación dinámica se sigue que para este caso particular
$J_{N}(x)=\beta^{N}(x_N)^{1-\gamma}$

luego, para $k= N-1$
$$J_{N-1}=\min_{a\in A(x)}\{\beta^{N-1}(a)^{1-\gamma} + \beta^{N}(1+r)^{1-\gamma}(x-a)^{1-\gamma}\}$$
derivando con respecto a $a$ obtenemos 
$$(1-\gamma)\beta^{N-1}a^{-\gamma}- \beta^{N}(1+r)^{1-\gamma}(x-a)^{-\gamma}$$
después igualando a cero, obtenemos
$$(1-\gamma)\beta^{N-1}a^{-\gamma}-\beta(1+r)^{1-\gamma}(x-a)^{-\gamma}=0$$
entonces 
$$(\cfrac{x-a}{a})^\gamma=\beta(1+r)^{1-\gamma}$$
$$\cfrac{x-a}{a}=[\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}$$
 por lo que el mínimo se alcanza en 
$$a_0=\dfrac{x}{[\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}+1}$$
sustituimos y obtenhemos la siguiente expresión para $J_{N--1}$
$$J_{N-1}(x)=\beta^{N-1}(a_0)^{1-\gamma}+\beta^N(1+r)^{1-\gamma}(x-a_0)^{1-\gamma}$$
Luego,
$$J_{N-1}(x)=\dfrac{\beta^{N-1}x^{1-\gamma}}{([\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}+1)^{1-\gamma}}+\beta^N(1+r)^{1-\gamma}\left[\dfrac{x[\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}}{\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}+1}\right]^{1-\gamma}$$
entonces
$$J_{N-1}(x)=\beta^{N-1}x^{1-\gamma}\left[[\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}+1\right]^{\gamma}$$
finalmente
$$J_{N-1}(x)=A_{N-1}\beta^{N-1}x^{1-\gamma}$$
donde
$$A_{N-1}=\left(1+((1+r)^{1-\gamma}\beta)^{\frac{1}{\gamma}}\right)^{\gamma}$$
Supongamos que se cumple para $n=k+1$, entonces $J_{k+1}(x)=A_{k+1}\beta^{k+1}x^{1-\gamma}$. Ahora
$$J_k(x)=\min_{0<a<x}\left\{\beta^ka^{1-\gamma}+A_{k+1}\beta^{k+1}(1+r)^{1-\gamma}(x-a)^{1-\gamma} \right\}$$
Derivando e igualando a 0 para encontrar el mínimo
$$(1-\gamma)\beta^ka^{-\gamma}-A_{k+1}\beta^{k+1}(1+\gamma)^{1-\gamma}(1-\gamma)(x-a)^{-\gamma}=0$$
Notemos que 
$$(1-\gamma)\beta^k\left[ a^{-\gamma}-A_{k+1}\beta(1+r)^{1-\gamma}(x-a)^{-\gamma}  \right]=0$$
$$a^{-\gamma}-A_{k+1}\beta(1+r)^{1-\gamma}(x-a)^{-\gamma}=0$$
entonces
$$\left(\dfrac{x-a}{a}\right)^{\gamma}=A_{k+1}\beta(1+\gamma)^{1-\gamma}$$
Despejando $a$, obtenemos que el mínimo se alcanza en 
$$a=\dfrac{x}{\left[ A_{k+1}\beta(1+r)^{1-\gamma}  \right]^{\frac{1}{\gamma}}+1}$$
A este punto lo renombramos como $a_0$ y sustituimos
$$J_k(x)=\dfrac{\beta^kx^{1-\gamma}}{\left[\left[ A_{k+1}\beta(1+r)^{1-\gamma}  \right]^{\frac{1}{\gamma}}+1\right]^{1-\gamma}}+\dfrac{A_{k+1}\beta^{k+1}(1+r)^{1-\gamma}\left(x[A_{k+1}\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}\right)^{1-\gamma}}{\left[\left[ A_{k+1}\beta(1+r)^{1-\gamma}  \right]^{\frac{1}{\gamma}}+1\right]^{1-\gamma}}$$
Desarrollando y reacomodando términos llegamos a
$$\dfrac{\beta^kx^{1-\gamma}\left( 1+[A_{k+1}\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}  \right)}{\left(1+[A_{k+1}\beta(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}  \right)^{1-\gamma}}=\beta^kx^{1-\gamma}A_k$$

## Exercise 5
**Exercise 5** Consider now the infinite–horizon version of the above consumption–saving problem.

1. Write down the associated Bellman equation.
2. Argue why a solution to the Bellman equation should be of the form
$$v(x)=cx^{1-\gamma}$$, where $c$ is constant. Find the constant and the stationary optimal policy.

*Solución*

$$cx^{1-\gamma}=\min\left\{ a^{1-\gamma}+\beta c(1+r)^{1-\gamma}(x-a)^{1-\gamma} \right\}$$
Derivando respecto a $a$ e igualando a 0
$$(1-\gamma)a^{-\gamma}-\beta c(1+r)^{1-\gamma}(1-\gamma)(x-a)^{-\gamma}=0$$
$$(1-\gamma)\left[ a^{-\gamma}-\beta c(1+r)^{1-\gamma}(x-a)^{-\gamma}\right]=0$$
$$a^{-\gamma}-\beta c(1+r)^{1-\gamma}(x-a)^{-\gamma}=0$$
Despenjando $a$
$$\left(\dfrac{x-a}{a}\right)^{\gamma}=\beta c(1+r)^{1-\gamma}$$
$$x=a\left[\beta c(1+r)^{1-\gamma} \right]^{\frac{1}{\gamma}}+a$$
$$a_0=a=\dfrac{x}{\left[\beta c(1+r)^{1-\gamma} \right]^{\frac{1}{\gamma}}+1}$$
Sustituyendo $a_0$, así
$$cx^{1-\gamma}=\dfrac{x^{1-\gamma}+\beta c(1+r)^{1-\gamma}x^{1-\gamma}\left[ (\beta c (1+r)^{1-\gamma})^{\frac{1}{\gamma}} \right]^{1-\gamma}}{\left[ (\beta c (1+r)^{1-\gamma})^{\frac{1}{\gamma}} +1\right]^{1-\gamma}}$$
$$cx^{1-\gamma}=x^{1-\gamma}\left[1+[\beta c (1+r)^{1-\gamma}]^{\frac{1}{\gamma}}\right]^{\gamma}$$
entonces
$$cx^{1-\gamma}=x^{1-\gamma}\left[1+[\beta c(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}\right]^{\gamma}$$
$$c=\left[1+[\beta c(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}\right]^{\gamma}$$
$$c^{\frac{1}{\gamma}}=1+[\beta c(1+r)^{1-\gamma}]^{\frac{1}{\gamma}}$$
$$c^{\frac{1}{\gamma}}=\left[1-\beta^{\frac{1}{\gamma}}(1+r)^{\frac{1-\gamma}{\gamma}}\right]$$
$$c^{\frac{1}{\gamma}}=\dfrac{1}{1-\beta^{\frac{1}{\gamma}}(1+r)^{\frac{1-\gamma}{\gamma}}}$$
de donde 
$$
c=\left[\dfrac{1}{1-\beta^{\frac{1}{\gamma}}(1+r)^{\frac{1-\gamma}{\gamma}}}\right]^{\gamma}
$$

## Exercise 6
**Exercise 6** Let $\{\xi_k\}$ be a sequence of iid random variables such that $E[\xi]=0$ and $E[\xi^2]=d$. Consider the dynamics
$$x_{k+1}=x_k+a_k+\xi_k,\hspace{0.5cm}k=0,1,2,...,$$
and the discounted cost
$$E\sum \beta^k(a_k^2+x_k^2).$$
i. Write down the associated Bellman equation.

ii. Conjecture that the solution to the Bellman equation takes the form $v(x)=ax^2+b$, where $a$ and $b$ are constant.

iii. Determine the constants $a$ and $b$. 

iv. Conjecture that the solution to the Bellman equation takes the form $v(x)=ax^2+b$, where $a$ y $b$ are constant. Determine the constants $a$ and $b$.
*Solución* Sea $A=a$ y $B=b$, entonces
$$Ax^2+B=\min_{a\in A(x)}\{a^2+x^2+\beta E[A(x+a+\xi)^2+B]\}$$
$$=\min_{a\in A(x)}\{a^2+x^2+\beta AE[(x+a+\xi)^2]+\beta B\}$$
$$=\min_{a\in A(x)}\{a^2+x^2+A\beta E[x^2+2ax+a^2+2(x+a)\xi+\xi^2]+\beta B\}$$
$$=\min_{a\in A(x)}\{a^2+x^2+A\beta x^2+2axA\beta+A\beta a^2+A\beta d+\beta B\}$$
Derivando respecto a $a$
$$2a+2xA\beta+2A\beta a=0$$
Despejando, obtenemos que el mínimo se alcanza en 
$$a=\dfrac{-xA\beta}{1+A\beta}$$
Entonces
$$Ax^2+B=\dfrac{(xA\beta)^2}{(a+A\beta)^2}+x^2+\beta E\left[A\left(\dfrac{x}{1+A\beta}+\xi \right)^2 \right]+\beta B$$
$$=\dfrac{x^2A^2\beta^2}{(1+A\beta)^2}+x^2+A\beta E\left[ \dfrac{x^2}{(1+A\beta)^2}+\dfrac{2x\xi}{1+A\beta}+\xi^2\right]+\beta B$$

$$=\dfrac{x^2A\beta(1+A\beta)}{(1+A\beta)^2}+x^2+A\beta d+\beta B$$
$$=x^2\left(1+\dfrac{A\beta}{1+A\beta}\right)+A\beta d+\beta B$$
Por lo que 
$$A=1+\dfrac{A\beta}{1+A\beta}$$
$$B=\dfrac{A\beta d}{1-\beta}$$
Ahora 
$$A=\dfrac{1+2A\beta}{1+A\beta}$$
$$A^2\beta+A(1-2\beta)-1=0$$
Resolviendo
$$A=\dfrac{-1+2\beta\pm \sqrt{4\beta^2+1}}{2\beta}$$
